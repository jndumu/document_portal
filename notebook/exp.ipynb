{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1003c359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026e868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1830d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bc19e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c65648",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116753b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The capital of Cameroon is **Yaoundé**.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"whats the capital of cameroon?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec25f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "432b34c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc91e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_embedding=embedding_model.embed_query(\"what's the capital of Paris?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef26065",
   "metadata": {},
   "source": [
    "Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3da430e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "616ed63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "758403ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b37becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=os.path.join(os.getcwd(), \"data\", \"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4539d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5bd62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7c9cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fd07b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd43b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3da76e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f067cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'}, page_content='Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dee96c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9f02b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2023-07-20T00:30:36+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2023-07-20T00:30:36+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 77,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8b62fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "545877d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a82c88ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vector=embedding_model.embed_documents(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b8d096b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_model.embed_documents(docs[0].page_content)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8636eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorstore=Chroma.from_documents(docs, embedding_model)\n",
    "vectorstore=FAISS.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "872b71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc=vectorstore.similarity_search(\"llama2 finetunning benchmark experiments.\",k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9d222ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='d68119a0-dc37-4da2-b87f-6d20c60aaf38', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'),\n",
       " Document(id='9c72739f-7348-4c69-aa39-f4b8ca501d76', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 75, 'page_label': '76'}, page_content='small delta (-0.9) between the \"clean\" subset performance and the sampling mean. No other dataset (for any\\nchoice ofL) appears to have benefitted from dataset contamination, and we omit results from these datasets\\nfor conciseness.\\n76'),\n",
       " Document(id='8b0396fd-f27c-4f7a-b592-848585e5bdc9', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(id='d5379317-5fa6-4503-9dce-859608c9a4e6', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 70, 'page_label': '71'}, page_content='65B 14.27 31.59 21.90 14.89 23.51 22.27 17.16 18.91 28.40 19.32 28.71 22.00 20.03\\nLlama 2\\n7B 16.53 31.15 22.63 15.74 26.87 19.95 15.79 19.55 25.03 18.92 21.53 22.34 20.20\\n13B 21.29 37.25 22.81 17.77 32.65 24.13 21.05 20.19 35.40 27.69 26.99 28.26 23.84\\n34B 16.76 29.63 23.36 14.38 27.43 19.49 18.54 17.31 26.38 18.73 22.78 21.66 19.04\\n70B 21.29 32.90 25.91 16.92 30.60 21.35 16.93 21.47 30.42 20.12 31.05 28.43 22.35\\nFine-tuned\\nChatGPT 0.23 0.22 0.18 0 0.19 0 0.46 0 0.13 0 0.47 0 0.66'),\n",
       " Document(id='4bc04ae7-a72d-4870-9580-6871ee8dc254', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 6, 'page_label': '7'}, page_content='models internally. For these models, we always pick the best score between our evaluation framework and\\nany publicly reported results.\\nIn Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety\\nbenchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The\\nresults for all the individual benchmarks are available in Section A.2.2.'),\n",
       " Document(id='6fd1cd77-c336-4828-961e-d145d1826458', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 1, 'page_label': '2'}, page_content='Contents\\n1 Introduction 3\\n2 Pretraining 5\\n2.1 Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.3 Llama 2Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n3 Fine-tuning 8\\n3.1 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9'),\n",
       " Document(id='96e92e1c-c571-443c-9a8f-2993c2df0f2b', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 48, 'page_label': '49'}, page_content='Human-Eval and MBPP respectively. For pass@100 and pass@80 scores, we use a temperature of 0.8 and\\ntop-p=0.95. For pass@1 scores, we use a temperature of 0.1 and top-p=0.95.\\n49'),\n",
       " Document(id='81240dd1-9f63-48ec-8da1-a6abdbc811f8', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 2, 'page_label': '3'}, page_content='be on par with some of the closed-source models, at least on the human evaluations we performed (see\\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving'),\n",
       " Document(id='853238a5-47f4-4925-90a9-136ae592a4e0', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 72, 'page_label': '73'}, page_content='Llama 2\\n7B 0.15 0.30 0.12 0.35 0.25 0.43 0.18 0.38 0.16 0.12 0.29 -0.1313B 0.14 0.35 0.23 0.29 0.23 0.57 0.20 0.52 0.22 0.12 0.29 -0.1734B 0.12 0.16 0.18 0.36 0.35 0.52 0.10 0.54 0.28 0.11 0.30 -0.1970B 0.16 0.21 0.17 0.35 0.30 0.60 0.18 0.67 0.26 0.12 0.30 -0.10\\nFine-tuned\\nChatGPT 0.15 0.22 0.05 0.24 0.31 0.35 0.09 0.42 0.19 0.09 0.23 0.06MPT-instruct 7B 0.13 0.29 0.12 0.34 0.35 0.53 0.28 0.56 0.27 0.02 0.32 -0.12Falcon-instruct 7B 0.11 0.21 0.21 0.28 0.34 0.23 0.31 0.45 0.23 0.22 0.29 -0.27'),\n",
       " Document(id='12282b39-5110-43ab-a9d3-92923bbef5cb', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 43, 'page_label': '44'}, page_content='Ba. Large language models are human-level prompt engineers. InThe Eleventh International Conference on\\nLearning Representations, 2022.\\n44')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "912398d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'small delta (-0.9) between the \"clean\" subset performance and the sampling mean. No other dataset (for any\\nchoice ofL) appears to have benefitted from dataset contamination, and we omit results from these datasets\\nfor conciseness.\\n76'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45f0bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc=vectorstore.similarity_search(\"llama2 finetunning benchmark experiments\",k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "754dbc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'small delta (-0.9) between the \"clean\" subset performance and the sampling mean. No other dataset (for any\\nchoice ofL) appears to have benefitted from dataset contamination, and we omit results from these datasets\\nfor conciseness.\\n76'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1b3082ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrival=vectorstore.as_retriever(search_kwargs={\"k\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d225db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='d68119a0-dc37-4da2-b87f-6d20c60aaf38', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'),\n",
       " Document(id='9c72739f-7348-4c69-aa39-f4b8ca501d76', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 75, 'page_label': '76'}, page_content='small delta (-0.9) between the \"clean\" subset performance and the sampling mean. No other dataset (for any\\nchoice ofL) appears to have benefitted from dataset contamination, and we omit results from these datasets\\nfor conciseness.\\n76'),\n",
       " Document(id='8b0396fd-f27c-4f7a-b592-848585e5bdc9', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(id='d5379317-5fa6-4503-9dce-859608c9a4e6', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 70, 'page_label': '71'}, page_content='65B 14.27 31.59 21.90 14.89 23.51 22.27 17.16 18.91 28.40 19.32 28.71 22.00 20.03\\nLlama 2\\n7B 16.53 31.15 22.63 15.74 26.87 19.95 15.79 19.55 25.03 18.92 21.53 22.34 20.20\\n13B 21.29 37.25 22.81 17.77 32.65 24.13 21.05 20.19 35.40 27.69 26.99 28.26 23.84\\n34B 16.76 29.63 23.36 14.38 27.43 19.49 18.54 17.31 26.38 18.73 22.78 21.66 19.04\\n70B 21.29 32.90 25.91 16.92 30.60 21.35 16.93 21.47 30.42 20.12 31.05 28.43 22.35\\nFine-tuned\\nChatGPT 0.23 0.22 0.18 0 0.19 0 0.46 0 0.13 0 0.47 0 0.66'),\n",
       " Document(id='4bc04ae7-a72d-4870-9580-6871ee8dc254', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 6, 'page_label': '7'}, page_content='models internally. For these models, we always pick the best score between our evaluation framework and\\nany publicly reported results.\\nIn Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety\\nbenchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The\\nresults for all the individual benchmarks are available in Section A.2.2.'),\n",
       " Document(id='6fd1cd77-c336-4828-961e-d145d1826458', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 1, 'page_label': '2'}, page_content='Contents\\n1 Introduction 3\\n2 Pretraining 5\\n2.1 Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.3 Llama 2Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n3 Fine-tuning 8\\n3.1 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9'),\n",
       " Document(id='96e92e1c-c571-443c-9a8f-2993c2df0f2b', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 48, 'page_label': '49'}, page_content='Human-Eval and MBPP respectively. For pass@100 and pass@80 scores, we use a temperature of 0.8 and\\ntop-p=0.95. For pass@1 scores, we use a temperature of 0.1 and top-p=0.95.\\n49'),\n",
       " Document(id='81240dd1-9f63-48ec-8da1-a6abdbc811f8', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 2, 'page_label': '3'}, page_content='be on par with some of the closed-source models, at least on the human evaluations we performed (see\\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving'),\n",
       " Document(id='853238a5-47f4-4925-90a9-136ae592a4e0', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 72, 'page_label': '73'}, page_content='Llama 2\\n7B 0.15 0.30 0.12 0.35 0.25 0.43 0.18 0.38 0.16 0.12 0.29 -0.1313B 0.14 0.35 0.23 0.29 0.23 0.57 0.20 0.52 0.22 0.12 0.29 -0.1734B 0.12 0.16 0.18 0.36 0.35 0.52 0.10 0.54 0.28 0.11 0.30 -0.1970B 0.16 0.21 0.17 0.35 0.30 0.60 0.18 0.67 0.26 0.12 0.30 -0.10\\nFine-tuned\\nChatGPT 0.15 0.22 0.05 0.24 0.31 0.35 0.09 0.42 0.19 0.09 0.23 0.06MPT-instruct 7B 0.13 0.29 0.12 0.34 0.35 0.53 0.28 0.56 0.27 0.02 0.32 -0.12Falcon-instruct 7B 0.11 0.21 0.21 0.28 0.34 0.23 0.31 0.45 0.23 0.22 0.29 -0.27'),\n",
       " Document(id='12282b39-5110-43ab-a9d3-92923bbef5cb', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\Dell\\\\Desktop\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 43, 'page_label': '44'}, page_content='Ba. Large language models are human-level prompt engineers. InThe Eleventh International Conference on\\nLearning Representations, 2022.\\n44')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrival.invoke(\"llama2 finetunning benchmark experiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61c0e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    Answer the question base the context provided below. If the context does not provide enouh information, respond\n",
    "    with 'i do not have sufficient information to provide an answer.'\n",
    "\n",
    "    context: {context}\n",
    "    question: {question}\n",
    "    Answer:\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7735a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0faccdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\",\"question\"]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a99771f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"\\n    Answer the question base the context provided below. If the context does not provide enouh information, respond\\n    with 'i do not have sufficient information to provide an answer.'\\n\\n    context: {context}\\n    question: {question}\\n    Answer:\\n\\n\\n\\n\")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f6a808a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c271f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser= StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "209a97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "403731ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "437c2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retrival | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e398c3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to figure out how to answer the question about the Llama2 fine-tuning benchmark experiments based on the provided context. Let me start by reading through the context carefully.\n",
      "\n",
      "Looking at the context, there are several tables and some text. The question is asking about the fine-tuning benchmark experiments for Llama2. I remember that fine-tuning usually involves taking a pre-trained model and training it further on a specific task or dataset to improve performance.\n",
      "\n",
      "In the context, I see a section labeled \"Fine-tuned\" with entries like \"ChatGPT\" and \"MPT-instruct 7B\". There's also a table that mentions \"Fine-tuned\" models, including ChatGPT and others. Another table shows performance metrics for these models, with numbers like 78.46, 79.92, etc. \n",
      "\n",
      "I should check if there's a specific table that details the benchmark results for the fine-tuned Llama2 models. I notice that in the context, there's a mention of \"Table 3\" which summarizes overall performance on grouped academic benchmarks, including fine-tuned models. Then there's \"Table 20\" which shows performance on standard benchmarks for both pre-trained and fine-tuned models, including Llama2-Chat models.\n",
      "\n",
      "I should also look for any text that explains the experiments. The context mentions that fine-tuning involved models like ChatGPT and MPT-instruct, and that the results are part of the benchmarks. It also talks about improvements in performance metrics for these models.\n",
      "\n",
      "Wait, the context also includes a section after the tables where it describes the pretraining approach, but that's more about the initial training, not fine-tuning. However, the mention of using different instruction-tuned models like ChatGPT and MPT-instruct suggests that fine-tuning was done to adapt the models for specific tasks.\n",
      "\n",
      "So, putting it together, the fine-tuned Llama2 models, such as ChatGPT and others, were evaluated on various benchmarks, and their performance is detailed in tables like Table 3 and Table 20. These tables show metrics like percentages and other scores, indicating how well the models performed after fine-tuning.\n",
      "\n",
      "I should make sure to mention that the fine-tuning process involved adapting the models to specific tasks, and that their performance was benchmarked and compared, showing improvements in certain areas. I might need to reference the specific tables and the metrics they report.\n",
      "\n",
      "I should structure the answer to first state that the context provides information on fine-tuned models, then mention the specific models evaluated, and reference the tables that show their benchmark performance. I should also note any key improvements or highlights from these experiments.\n",
      "\n",
      "Wait, in the context, there's a section labeled \"Fine-tuned\" with specific model names and numbers. For example, ChatGPT has scores like 0.23, 0.22, etc., and MPT-instruct 7B has other scores. Table 20 includes fine-tuned models like ChatGPT and MPT-instruct 7B with their respective performance metrics.\n",
      "\n",
      "So, the answer should include details about these models, their benchmark results, and any significant findings from the experiments. It's important to note that the context provides specific data points in tables which summarize the performance of these fine-tuned models across various benchmarks.\n",
      "\n",
      "I should also mention that the experiments involved comparing the fine-tuned models against pre-trained models, showing how fine-tuning can enhance performance in specific tasks or benchmarks.\n",
      "\n",
      "In summary, the answer should explain that the Llama2 fine-tuning benchmark experiments involved evaluating models like ChatGPT and MPT-instruct on various benchmarks, with results showing their performance improvements as detailed in the provided tables.\n",
      "</think>\n",
      "\n",
      "The Llama2 fine-tuning benchmark experiments involved evaluating several fine-tuned models, including ChatGPT and MPT-instruct 7B, across various benchmarks. These experiments demonstrated the effectiveness of fine-tuning in enhancing model performance on specific tasks. The results, as detailed in the provided tables, show that fine-tuned models achieved notable performance improvements compared to their pre-trained counterparts. For instance, ChatGPT exhibited strong performance with metrics such as 78.46 and 79.92, while MPT-instruct 7B also showed competitive results. These experiments highlight the benefits of fine-tuning for adapting models to particular tasks, as evidenced by the benchmark scores presented in the context.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"tell me about the llama2 finetunning benchmark experiments?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e9b712f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out the scaling trends for the reward model based on the provided context. Let me read through the context again to pick up any relevant information.\n",
      "\n",
      "First, the context mentions that Figure 6 reports trends showing that larger models obtain higher performance for a similar volume of data. That seems important. It also says that the scaling performance hasn't plateaued yet, which suggests that as the model size increases, the performance keeps improving without leveling off. They note that there's room for more improvement with more annotations, which implies that scaling up the data or model size could lead to better results.\n",
      "\n",
      "There's a part about the reward model accuracy being one of the most important factors, which might relate to how well the model scales. They talk about using a margin-based loss and its impact on reward score distributions. The margin term pushes the reward scores apart, which could affect how the model scales by making the rewards more distinct.\n",
      "\n",
      "They also mention that the reward models are initialized from pretrained chat models, which might mean that the scaling benefits from the pretraining. The training objectives include a ranking loss, which is used to enforce that chosen responses have higher scores than rejected ones. This could influence how the model scales as it's trained on more data.\n",
      "\n",
      "The context also discusses the final reward function, which includes a penalty term to prevent divergence from the original policy. This might help in maintaining stability as the model scales, preventing reward hacking where the model might exploit the reward function in unintended ways.\n",
      "\n",
      "There's a section on mixing training data from different sources and experimenting with settings for Helpfulness and Safety reward models. They found that fine-tuning on rejection sampled data helps distill capabilities from larger models to smaller ones, which could be a scaling strategy.\n",
      "\n",
      "They also talk about sampling answers and using the best reward model to score them, which suggests that as they sample more, the maximum reward increases, indicating better scaling with more samples. The temperature parameter affects exploration, with higher temperatures leading to more diverse outputs, which might be beneficial for scaling.\n",
      "\n",
      "Putting this all together, the scaling trends seem positive. Larger models perform better, and there's potential for further improvement with more data. The use of margin-based loss and proper training objectives helps maintain the model's performance as it scales. The penalty term in the reward function and data mixing strategies also contribute to stable and effective scaling.\n",
      "\n",
      "I don't see any mention of negative scaling trends, like performance decreases or plateaus beyond a certain point. The context emphasizes that the scaling hasn't plateaued yet, so the trend is upwards.\n",
      "</think>\n",
      "\n",
      "The scaling trends for the reward model are positive. Larger models demonstrate higher performance with similar data volumes, and the scaling performance has not yet plateaued, indicating potential for further improvement with more data. The use of margin-based loss and effective training objectives contributes to maintaining performance as the model scales. Additionally, strategies like data mixing and fine-tuning on rejection samples, along with a penalty term in the reward function, support stable scaling. The ability to sample more and the role of temperature in exploration further enhance scaling capabilities. Overall, the trend is upwards, with increasing model size leading to better performance.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"can you tell me scaling trens for the reward model?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7705d43b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
